{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f607ce-f2c5-47de-9228-37f6b68a609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chunks/chunk_0.csv\n",
      "Created chunks/chunk_1.csv\n",
      "Created chunks/chunk_2.csv\n",
      "Created chunks/chunk_3.csv\n",
      "Created chunks/chunk_4.csv\n",
      "Created chunks/chunk_5.csv\n",
      "Created chunks/chunk_6.csv\n",
      "Created chunks/chunk_7.csv\n",
      "Created chunks/chunk_8.csv\n",
      "Created chunks/chunk_9.csv\n",
      "Created chunks/chunk_10.csv\n",
      "Created chunks/chunk_11.csv\n",
      "Created chunks/chunk_12.csv\n",
      "Created chunks/chunk_13.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def split_csv(file_path, output_dir, chunk_size):\n",
    "    \"\"\"Split a CSV file into smaller chunks.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Read CSV in chunks\n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        chunk_file = os.path.join(output_dir, f\"chunk_{i}.csv\")\n",
    "        chunk.to_csv(chunk_file, index=False)\n",
    "        print(f\"Created {chunk_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"pennines_CN.csv\"\n",
    "    output_directory = \"chunks\"\n",
    "    chunk_size = 100000\n",
    "\n",
    "    split_csv(input_file, output_directory, chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eefd3a8-5bfa-410d-9315-c38d560049c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. File saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "\n",
    "def extract_raster_values(channel_network, raster_path, from_crs=\"EPSG:4326\", to_crs=\"EPSG:32630\"):\n",
    "    \"\"\"Assign raster values to channel network data points after reprojecting coordinates.\"\"\"\n",
    "    \n",
    "    # Initialize the CRS transformer\n",
    "    transformer = Transformer.from_crs(from_crs, to_crs, always_xy=True)\n",
    "    \n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject coordinates to the raster CRS\n",
    "        lons = np.array(channel_network['longitude'])\n",
    "        lats = np.array(channel_network['latitude'])\n",
    "        reprojected_coords = np.array([transformer.transform(lon, lat) for lon, lat in zip(lons, lats)])\n",
    "        \n",
    "        # Convert reprojected coordinates to raster indices\n",
    "        rows, cols = src.index(reprojected_coords[:, 0], reprojected_coords[:, 1])\n",
    "        \n",
    "        # Read the raster values at these locations\n",
    "        raster_values = np.empty(len(rows))\n",
    "        for i, (row, col) in enumerate(zip(rows, cols)):\n",
    "            try:\n",
    "                raster_values[i] = src.read(1)[row, col]\n",
    "            except IndexError:\n",
    "                raster_values[i] = np.nan\n",
    "        \n",
    "    # Add the raster values to the channel network\n",
    "    channel_network['rocktype_id'] = raster_values\n",
    "    return channel_network\n",
    "\n",
    "# Paths to the raster file and channel network CSV\n",
    "raster_file = \"merged_bedrock.bil\"\n",
    "channel_network_csv = \"chunk_13.csv\"\n",
    "\n",
    "# Load the channel network\n",
    "channel_network = pd.read_csv(channel_network_csv)\n",
    "\n",
    "# Process the data\n",
    "channel_network_updated = extract_raster_values(channel_network, raster_file)\n",
    "\n",
    "# Save the updated channel network\n",
    "channel_network_updated.to_csv('processed_chunks_13.csv', index=False)\n",
    "print(\"Processing complete. File saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8592d2da-1cd0-4deb-b5b9-7261b331f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1592/882890032.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_data = pd.concat([merged_data, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# List all CSV files with the specified prefix\n",
    "csv_files = glob.glob('processed_chunks_*.csv')\n",
    "\n",
    "# Read the first CSV file to set the column names\n",
    "first_file = pd.read_csv(csv_files[0])\n",
    "column_names = first_file.columns.tolist()\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "merged_data = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Loop through the CSV files and concatenate them into the merged DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_data.to_csv('pennines_CN_with_rocktype.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec3416e7-1392-41ce-8b64-e83b339fb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load the channel network and junction data\n",
    "channel_network = pd.read_csv('pennines_CN_with_rocktype.csv')  # Your channel network file\n",
    "junctions = pd.read_csv('pennines_sin_grad_filtered.csv')  # Your junction file\n",
    "\n",
    "# Merge channel network with junctions to find the segments between junctions\n",
    "merged_data = pd.merge(channel_network, junctions, \n",
    "                       left_on='receiver_JI', right_on='receiver_junction', \n",
    "                       suffixes=('_segment', '_junction'))\n",
    "\n",
    "# Group by 'Junction Index' and 'receiver_JI' to find segments\n",
    "segments = merged_data.groupby(['Junction Index', 'receiver_JI']).agg(list).reset_index()\n",
    "\n",
    "# Function to count rocktype along a channel segment\n",
    "def count_rocktype(segment, network_data):\n",
    "    rocktype_counts = Counter(segment['rocktype_id'])\n",
    "    most_common_rocktype_id = rocktype_counts.most_common(1)[0][0]\n",
    "    return most_common_rocktype_id\n",
    "\n",
    "# Apply the function to each segment\n",
    "segments['most_common_rocktype_id'] = segments.apply(count_rocktype, axis=1, network_data=channel_network)\n",
    "\n",
    "# Map the most common rocktype_id back to the junctions\n",
    "junctions = junctions.merge(segments[['Junction Index', 'most_common_rocktype_id']], \n",
    "                            left_on='junction', right_on='Junction Index', \n",
    "                            how='left')\n",
    "\n",
    "# Drop duplicate 'Junction Index' entries\n",
    "junctions = junctions.drop_duplicates(subset=['junction'])\n",
    "\n",
    "# Save the updated junctions data with assigned rocktype_id\n",
    "junctions.to_csv('pennines_sin_grad_geol_fil.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823dbb6-5165-4174-abc9-238fd4897c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
